I"'U<h1 id="talks">Talks</h1>

<p><strong>NB:</strong> Click speaker and title to toggle abstract.</p>

<!-- Tab links -->
<div class="tab">
  <!-- <button> </button> -->
  <button class="tablinks" onclick="openDate(event, 'Aug3')" id="defaultOpen">August 3</button>
  <button class="tablinks" onclick="openDate(event, 'Aug4')">August 4</button>
  <button class="tablinks" onclick="openDate(event, 'Aug5')">August 5</button>
  <button class="tablinks" onclick="openDate(event, 'Aug6')">August 6</button>
  <button class="tablinks" onclick="openDate(event, 'Aug7')">August 7</button>
</div>

<!-- Tab content -->
<div id="Aug3" class="tabcontent">
    <table class="tg">
      <tr>
        <th class="tg-lboi" style="min-width:130px">Time</th>
        <th class="tg-lboi">Event</th>
      </tr>
      <tr>
        <td class="tg-lboi">8:50 - 9:00</td>
        <td class="tg-talk">Opening remarks</td>
      </tr>
      <tr>
        <td class="tg-lboi">9:00 - 9:45</td>
        <td class="tg-talk"> <details> <summary><a>Stephane Chretien</a>: Understanding interpolation in machine learning</summary><b>Abstract:</b>Recent progress in machine learning practice has lead to the conclusion that over-parametrisation was an essential ingredient in the success of deep neural networks. In this talk, I will survey the recent achievements by many authors from the applied mathematics community for unveiling the reasons why, contrarily to standard belief, overfitting is not the rule when the number of parameters largely exceeds the size of the training set. In particular we will explain the importance of the recently discovered "double descent phenomenon" by Belkin, Hsu, Ma and Mandal. We will end the presentation with a new proposal for studying this intriguing phenomenon and present novel results in this direction obtained jointly with E. Caron in the finite sample and subGaussian setting. </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">9:45 - 10:30</td>
        <td class="tg-talk"> <details> <summary><a>Johannes Schmidt-Hieber</a>: Overparametrization and the bias-variance dilemma</summary><b>Abstract:</b> 	For several machine learning methods such as neural networks, good generalisation performance has been reported in the overparametrized regime. In view of the classical bias-variance trade-off, this behaviour is highly counterintuitive. The talk summarizes recent theoretical results on overparametrization and the bias-variance trade-off. This is joint work with Alexis Derumigny. </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:00 - 13:45</td>
        <td class="tg-talk"> <details> <summary><a>Lenaic Chizat</a>: Analysis of Gradient Descent on Wide Two-Layer ReLU Neural Networks </summary><b>Abstract:</b> 	In this talk, we propose an analysis of gradient descent on wide two-layer ReLU neural networks that leads to sharp characterizations of the learned predictor and strong generalization performances. The main idea is to study the dynamics when the width of the hidden layer goes to infinity, which is a Wasserstein gradient flow. While this dynamics evolves on a non-convex landscape, we show that its limit is a global minimizer if initialized properly. We also study the "implicit bias" of this algorithm when the objective is the unregularized logistic loss. We finally discuss what these results tell us about the generalization performance. This is based on joint work with Francis Bach.  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:45 - 14:30</td>
        <td class="tg-talk"> <details> <summary><a>Varun Kanade</a>: Implicit Regularization Properties Early-Stopped Gradient-based Algorithms	  </summary><b>Abstract:</b> In this talk, we will discuss implicit regularization properties of gradient-based algorithms. First, we will discuss how gradient descent when applied to unpenalized least squares regression solves the problem of reconstructing a sparse signal from an underdetermined system of linear measurements under the restricted isometry assumption. We present a statitically and computationally optimal algorithm for this problem in this setting and compare the behaviour to the explicit l_1 penalized versions. Next, we will discuss statistical guarantees on excess risk of early-stopped unconstrained mirror descent when applied to the unregularized empirical risk with squared loss on linear models. We will discuss the connection to offset Rademacher complexities and use our method to derive short proofs for new and existing results in the recent literature. This is based on joint work with Patrick Rebeschini and Tomas Vaskevicius. </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">15:30 - 16:15</td>
        <td class="tg-talk"> <details> <summary><a>Weinan E</a>: Machine learning from a continuous viewpoint </summary><b>Abstract:</b> Modern machine learning is characterized by two distinctive features: It can be very powerful and it can be quite fragile. The former does not need any elaboration. The latter refers to the fact that the performance of modern machine learning algorithms depends sensitively on the choice of the hyperparameters. This talk centers on continuous formulations of machine learning that are ``well-posed''. We formulate machine learning and the associated optimization process as well-behaved variational and PDE-like problems, and demonstrate that some of the most popular modern machine learning algorithms can be recovered as particular discretizations of these continuous problems. We demonstrate that the performance of the algorithms obtained this way tends to be more robust with the different choices of the hyperparameters. We also discuss how to develop new algorithms under this framework. </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">16:15 - 17:00</td>
        <td class="tg-talk"> <details> <summary><a>Brynjulf Owren</a>: Structure preservation in (some) deep learning architecture </summary><b>Abstract:</b> 	A deep neural network model consists a large number of layers, each with a number of parameters associated to them. In supervised learning, these parameters are optimised to match training data in the best possible way. The data are propagated through the layers by nonlinear transformations, and in an important subclass of models (ResNet) the transformation can be seen as the numerical flow of a certain type of continuous vector field. Ruthotto and Haber (2017) as well as Cheng et al. have experimented in using different type of vector fields to improve the deep learning model. In particular it is of interest that the trained model has good long time behaviour and are stable in the deep limit, when the number of layers tends to infinity. The models presented in the literature have certain builtin structural properties, they can for instance be gradient flows or Hamiltonian vector fields. A difficulty is however that the models are not autonomous and therefore it is less clear what their flows actually preserve. Starting from such ResNet vector fields, we shall discuss their properties and derive some new nonlinear stability bounds. The long time behaviour of these neural ODE flows is important in the generalisation mode, i.e. after the model has been trained. But also in the training algorithm itself,  structure preserving numerical schemes are important. In deep learning models, the use of gradient flows for optimisation is prevalent, and there exists a number of different algorithms that can be used, some of them can be interpreted as approximations of the flow of certain vector fields with dissipations, such as conformal Hamiltonian systems. If time permits, we will briefly discuss also these algorithms and in particular the need for and efficiency of regularisation. Joint work with: Martin Benning, Elena Celledoni, Matthias Ehrhardt, Christian Etmann, Robert McLachlan, Carola-Bibiane Sch√∂nlieb and Ferdia Sherry. </details> </td>
      </tr>
      
    </table>
</div>

<div id="Aug4" class="tabcontent">
    <table class="tg">
         <tr>
        <td class="tg-lboi">9:00 - 9:45</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">9:45 - 10:30</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:00 - 13:45</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:45 - 14:30</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">15:30 - 16:15</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">16:15 - 17:00</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      </table>
</div>

<div id="Aug5" class="tabcontent">
    <table class="tg">
      <tr>
        <td class="tg-lboi">9:00 - 9:45</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">9:45 - 10:30</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:00 - 13:45</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:45 - 14:30</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">15:30 - 16:15</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">16:15 - 17:00</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
    </table>
</div>

<div id="Aug6" class="tabcontent">
    <table class="tg">
      <tr>
        <td class="tg-lboi">9:00 - 9:45</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">9:45 - 10:30</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:00 - 13:45</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:45 - 14:30</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">15:30 - 16:15</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">16:15 - 17:00</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
    </table>
</div>

<div id="Aug7" class="tabcontent">
    <table class="tg">
      <tr>
        <td class="tg-lboi">9:00 - 9:45</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">9:45 - 10:30</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:00 - 13:45</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">13:45 - 14:30</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">15:30 - 16:15</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
      <tr>
        <td class="tg-lboi">16:15 - 17:00</td>
        <td class="tg-talk"> <details> <summary><a>SPEAKER</a>:   TITLE  </summary><b>Abstract:</b> ABSTRACT	  </details> </td>
      </tr>
    </table>
</div>

<h1 id="posters">Posters</h1>

<table>
  <thead>
    <tr>
      <th style="text-align: center">ID</th>
      <th style="text-align: left">Presenter</th>
      <th style="text-align: left">Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">001</td>
      <td style="text-align: left">Eliot Ayache (University of Bath)</td>
      <td style="text-align: left">Machine learning applications in High-Energy Time-Domain Astrophysics</td>
    </tr>
    <tr>
      <td style="text-align: center">002</td>
      <td style="text-align: left">Pasquale Cascarano (Universit√† di Bologna)</td>
      <td style="text-align: left">Deep Plug-and-Play Gradient Method for Super-Resolution</td>
    </tr>
    <tr>
      <td style="text-align: center">003</td>
      <td style="text-align: left">Eric Baruch Gutierrez Castillo	(University of Bath)</td>
      <td style="text-align: left">On Primal-Dual Algorithms for Nonsmooth Large-Scale Machine Learning</td>
    </tr>
    <tr>
      <td style="text-align: center">004</td>
      <td style="text-align: left">Dongdong Chen	(University of Edinburgh)</td>
      <td style="text-align: left">Deep Plug-and-Play Network for Compressive MRF reconstruction</td>
    </tr>
    <tr>
      <td style="text-align: center">005</td>
      <td style="text-align: left">Maria Colomba Comes	(University of Rome Tor Vergata)</td>
      <td style="text-align: left">Deep Prior for Cells Video Restoration</td>
    </tr>
    <tr>
      <td style="text-align: center">006</td>
      <td style="text-align: left">Jacob Deasy	(University of Cambridge)</td>
      <td style="text-align: left">Closed-form differential entropy of a multi-layer perceptron variant</td>
    </tr>
    <tr>
      <td style="text-align: center">007</td>
      <td style="text-align: left">Margaret Duff	(University of Bath)</td>
      <td style="text-align: left">Solving Inverse Imaging Problems with Generative</td>
    </tr>
    <tr>
      <td style="text-align: center">008</td>
      <td style="text-align: left">David Fernandes	(University of Bath)</td>
      <td style="text-align: left">Unsupervised Learning with GPs and SDEs</td>
    </tr>
    <tr>
      <td style="text-align: center">009</td>
      <td style="text-align: left">Allen Hart	(University of Bath)</td>
      <td style="text-align: left">Machine Learning Models</td>
    </tr>
    <tr>
      <td style="text-align: center">010</td>
      <td style="text-align: left">Allard Hendriksen	(Centrum Wiskunde &amp; Informatica (CWI), Amsterdam)</td>
      <td style="text-align: left">Noise2Inverse: Self-supervised deep convolutional denoising for linear inverse problems in imaging</td>
    </tr>
    <tr>
      <td style="text-align: center">011</td>
      <td style="text-align: left">Johannes Hertrich	(TU Berlin)</td>
      <td style="text-align: left">Parseval Proximal Neural Networks</td>
    </tr>
    <tr>
      <td style="text-align: center">012</td>
      <td style="text-align: left">Gabriele Incorvaia	(University of Manchester)</td>
      <td style="text-align: left">A deep learning application for Through-the-Wall Radar Imaging</td>
    </tr>
    <tr>
      <td style="text-align: center">013</td>
      <td style="text-align: left">Nicolas Keriven	(CNRS, GIPSA-lab)</td>
      <td style="text-align: left">coordinate inter-building electricity demand</td>
    </tr>
    <tr>
      <td style="text-align: center">014</td>
      <td style="text-align: left">Youngkyu Lee	 (KAIST)</td>
      <td style="text-align: left">A parareal neural network emulating parallel-in-time algorithm</td>
    </tr>
    <tr>
      <td style="text-align: center">015</td>
      <td style="text-align: left">Peter Math√©	(Weierstrass Institute)</td>
      <td style="text-align: left">Inverse learning in Hilbert scales</td>
    </tr>
    <tr>
      <td style="text-align: center">016</td>
      <td style="text-align: left">Janith Petangoda	(Imperial College London)</td>
      <td style="text-align: left">Transfer Learning: An application of Foliations</td>
    </tr>
    <tr>
      <td style="text-align: center">017</td>
      <td style="text-align: left">Benedek Plosz	(University of Bath)</td>
      <td style="text-align: left">Learning with unexplored priors systemises error propagation in Bayesian hierarchical modelling</td>
    </tr>
    <tr>
      <td style="text-align: center">018</td>
      <td style="text-align: left">Ilan Price (University of Oxford)</td>
      <td style="text-align: left">Trajectory growth through deep random ReLU networks</td>
    </tr>
    <tr>
      <td style="text-align: center">019</td>
      <td style="text-align: left">Abhishake Rastogi	(University of Potsdam)</td>
      <td style="text-align: left">Regularization schemes for statistical inverse problems</td>
    </tr>
    <tr>
      <td style="text-align: center">020</td>
      <td style="text-align: left">Salvatore Danilo Riccio	(Queen Mary University of London)</td>
      <td style="text-align: left">Robustness of Runge-Kutta networks against adversarial attacks</td>
    </tr>
    <tr>
      <td style="text-align: center">021</td>
      <td style="text-align: left">Paul Russell	(University of Bath)</td>
      <td style="text-align: left">Learning to solve Rubik‚Äôs cube</td>
    </tr>
    <tr>
      <td style="text-align: center">022</td>
      <td style="text-align: left">Malena Sabat√©	(University of Bath)</td>
      <td style="text-align: left">Iteratively Reweighted Flexible Krylov methods for Sparse Reconstruction <a href="Posters/MalenaSabate.png" target="_blank"><code class="highlighter-rouge">Poster</code></a> <a href="https://drive.google.com/file/d/1FXWYpcRW9axsPoBZtzzxEMPcqp9_YS6d/view?usp=sharing" target="_blank"><code class="highlighter-rouge">Video</code></a></td>
    </tr>
    <tr>
      <td style="text-align: center">023</td>
      <td style="text-align: left">Silvester Sabathiel	(NTNU Trondheim)</td>
      <td style="text-align: left">A computational model of learning to count in a multimodal, interactive environment</td>
    </tr>
    <tr>
      <td style="text-align: center">024</td>
      <td style="text-align: left">Ferdia Sherry	(University of Cambridge)</td>
      <td style="text-align: left">Equivariant neural networks for inverse problems in imaging</td>
    </tr>
    <tr>
      <td style="text-align: center">025</td>
      <td style="text-align: left">Giuseppe Ughi	(University of Oxford)</td>
      <td style="text-align: left">A Model-Based Derivative-Free Approach to Black-Box Adversarial Examples</td>
    </tr>
    <tr>
      <td style="text-align: center">026</td>
      <td style="text-align: left">Tiffany Vlaar	(University of Edinburgh)</td>
      <td style="text-align: left">Partitioned Integrators for Thermodynamic Parameterization of NNs</td>
    </tr>
    <tr>
      <td style="text-align: center">027</td>
      <td style="text-align: left">Xiaoyu Wang	(University of Cambridge)</td>
      <td style="text-align: left">Gradient-based training of non-smooth neural networks</td>
    </tr>
    <tr>
      <td style="text-align: center">028</td>
      <td style="text-align: left">Stefan Wild	(Argonne National Laboratory)</td>
      <td style="text-align: left">Zero-order optimization for learning: How long can you wait?</td>
    </tr>
    <tr>
      <td style="text-align: center">029</td>
      <td style="text-align: left">Kelvin Shuangjian Zhang (ENS Paris)</td>
      <td style="text-align: left">Wasserstein control of mirror Langevin Monte Carlo</td>
    </tr>
  </tbody>
</table>

<h1 id="public-lecture">Public Lecture</h1>

<p><a href="https://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane Sch√∂nlieb</a>, Professor at the University of Cambridge, will give a public lecture.</p>

<p>Title: Looking into the black box: how mathematics can help to turn deep learning inside out</p>

<p>Abstract: Deep learning has had a transformative impact on a wide range of tasks related to Artificial Intelligence, ranging from computer vision and speech recognition to playing games. Still, the inner workings of deep neural networks are far from clear, and designing and training them is seen as almost a black art. In this talk we will try to open this black box a little bit by using mathematical structure of neural networks described by so-called differential equations and mathematical optimisation. The talk is furnished with several examples in image analysis and computer vision, ranging from biomedical imaging to remote sensing.</p>

<p>Time: 18.00 BST, Thursday 6th August 2020.</p>

<p>Sign up <a href="https://docs.google.com/forms/d/e/1FAIpQLScuiUa6xDKhjHA1pJcXj-FcC8VihqnHJIYm2UqorWlQ0MC88g/viewform">here</a></p>

:ET